<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Deep Neural Net parameters and hyperparameters — hpar • automl</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha256-nAmazAk6vS34Xqo0BSrTb+abbtFlgsFK7NKSi6o7Y78=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/v4-shims.min.css" integrity="sha256-6qHlizsOWFskGlwVOKuns+D1nB6ssZrHQrNj1wGplHc=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/headroom.min.js" integrity="sha256-DJFC1kqIhelURkuza0AvYal5RxMtpzLjFhsnVIeuk+U=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Deep Neural Net parameters and hyperparameters — hpar" />
<meta property="og:description" content="List of Neural Network parameters and hyperparameters to train with gradient descent or particle swarm optimization
Not mandatory (the list is preset and all arguments are initialized with default value) but it is advisable to adjust some important arguments for performance reasons (including processing time)" />
<meta name="twitter:card" content="summary" />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">automl</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">1.2.9</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/automl.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/aboulaboul/automl">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Deep Neural Net parameters and hyperparameters</h1>
    
    <div class="hidden name"><code>hpar.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>List of Neural Network parameters and hyperparameters to train with gradient descent or particle swarm optimization<br />
Not mandatory (the list is preset and all arguments are initialized with default value) but it is advisable to adjust some important arguments for performance reasons (including processing time)</p>
    </div>


    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>modexec</th>
      <td><p>&#8216;trainwgrad&#8217; (the default value) to train with gradient descent (suitable for all volume of data)<br />
&#8216;trainwpso&#8217; to train using Particle Swarm Optimization, each particle represents a set of neural network weights (CAUTION: suitable for low volume of data, time consuming for medium to large volume of data)</p></td>
    </tr>
    <tr>
      <th>learningrate</th>
      <td><p>learningrate alpha (default value 0.001)<br />
#tuning priority 1</p></td>
    </tr>
    <tr>
      <th>beta1</th>
      <td><p>see below</p></td>
    </tr>
    <tr>
      <th>beta2</th>
      <td><p>&#8216;Momentum&#8217; if beta1 different from 0 and beta2 equal 0 )<br />
&#8216;RMSprop&#8217; if beta1 equal 0 and beta2 different from 0<br />
&#8216;adam optimization&#8217; if beta1 different from 0 and beta2 different from 0 (default)<br />
(default value beta1 equal 0.9 and beta2 equal 0.999)<br />
#tuning priority 2</p></td>
    </tr>
    <tr>
      <th>lrdecayrate</th>
      <td><p>learning rate decay value (default value 0, no learning rate decay, 1e-6 should be a good value to start with)<br />
#tuning priority 4</p></td>
    </tr>
    <tr>
      <th>chkgradevery</th>
      <td><p>epoch interval to run gradient check function (default value 0, for debug only)</p></td>
    </tr>
    <tr>
      <th>chkgradepsilon</th>
      <td><p>epsilon value for derivative calculations and threshold test in gradient check function (default 0.0000001)</p></td>
    </tr>
    <tr>
      <th>psoxxx</th>
      <td><p>see <a href='pso.html'>pso</a> for PSO specific arguments details</p></td>
    </tr>
    <tr>
      <th>costcustformul</th>
      <td><p>custom cost formula (default &#8216;&#8217;, no custom cost function)<br />
standard input variables: yhat (prediction), y (target actual value)<br />
custom input variables: any variable declared in hpar may be used via alias mydl (ie: hpar(list = (foo = 1.5)) will be used in custom cost formula as mydl$foo))<br />
result: J<br />
see &#8216;automl_train_manual&#8217; example using Mean Average Percentage Error cost function<br />
nb: X and Y matrices used as input into automl_train_manual or automl_train_manual functions are transposed (features in rows and cases in columns)</p></td>
    </tr>
    <tr>
      <th>numiterations</th>
      <td><p>number of training epochs (default value 50))<br />
#tuning priority 1</p></td>
    </tr>
    <tr>
      <th>seed</th>
      <td><p>seed for reproductibility (default 4)</p></td>
    </tr>
    <tr>
      <th>minibatchsize</th>
      <td><p>mini batch size, 2 to the power 0 for stochastic gradient descent (default 2 to the power 5)
#tuning priority 3</p></td>
    </tr>
    <tr>
      <th>layersshape</th>
      <td><p>number of nodes per layer, each nodes number initialize a hidden layer<br />
output layer nodes number, may be left to 0 it will be automatically set by Y matrix shape<br />
default value one hidden layer with 10 nodes: c(10, 0)<br />
#tuning priority 4</p></td>
    </tr>
    <tr>
      <th>layersacttype</th>
      <td><p>activation function for each layer; &#8216;linear&#8217; for no activation or &#8216;sigmoid&#8217;, &#8216;relu&#8217; or &#8216;reluleaky&#8217; or &#8216;tanh&#8217; or &#8216;softmax&#8217; (softmax for output layer only supported in trainwpso exec mode)<br />
output layer activation function may be left to &#8216;&#8217;, default value &#8216;linear&#8217; for regression, &#8216;sigmoid&#8217; for classification<br />
nb: layersacttype parameter vector must have same length as layersshape parameter vector<br />
default value c(&#8216;relu&#8217;, &#8216;&#8217;)<br />
#tuning priority 4</p></td>
    </tr>
    <tr>
      <th>layersdropoprob</th>
      <td><p>drop out probability for each layer, continuous value from 0 to less than 1 (give the percentage of matrix weight values to drop out randomly)<br />
nb: layersdropoprob parameter vector must have same length as layersshape parameter vector<br />
default value no drop out: c(0, 0)<br />
#tuning priority for regularization</p></td>
    </tr>
    <tr>
      <th>printcostevery</th>
      <td><p>epoch interval to test and print costs (train and cross validation cost: default value 10, for 1 test every 10 epochs)</p></td>
    </tr>
    <tr>
      <th>testcvsize</th>
      <td><p>size of cross validation sample, 0 for no cross validation sample (default 10, for 10 percent)</p></td>
    </tr>
    <tr>
      <th>testgainunder</th>
      <td><p>threshold to stop the training if the gain between last train or cross validation cost is smaller than the threshold, 0 for no stop test (default 0.000001)</p></td>
    </tr>
    <tr>
      <th>costtype</th>
      <td><p>cost type function name &#8216;mse&#8217; or &#8216;crossentropy&#8217; or &#8216;custom&#8217;<br />
&#8216;mse&#8217; for Mean Squared Error, set automatically for continuous target type<br />
&#8216;crossentropy&#8217; set automatically for binary target type<br />
&#8216;custom&#8217; set automatically if &#8216;costcustformul&#8217; different from &#8216;&#8217;</p></td>
    </tr>
    <tr>
      <th>lambda</th>
      <td><p>regularization term added to cost function (default value 0, no regularization)</p></td>
    </tr>
    <tr>
      <th>batchnor_mom</th>
      <td><p>batch normalization momentum for j and B (default value 0.9, no batch normalization if equal to 0)</p></td>
    </tr>
    <tr>
      <th>epsil</th>
      <td><p>epsilon the low value to avoid dividing by 0 or log(0) in cost function, etc ... (default value 1e-12)<br /></p></td>
    </tr>
    <tr>
      <th>verbose</th>
      <td><p>to display or not the costs and the shapes (default TRUE)</p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="see-also"><a class="anchor" href="#see-also"></a>See also</h2>

    <div class='dont-index'><p>Deep Learning specialization from Andrew NG on Coursera</p></div>

  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#arguments">Arguments</a></li>
      <li><a href="#see-also">See also</a></li>
    </ul>

  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Alex Boulangé.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.4.0.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


